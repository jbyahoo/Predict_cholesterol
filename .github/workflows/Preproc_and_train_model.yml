name: Cholesterol Prediction Pipeline

on: 
  push:
    branches: [main]
    paths:
      - 'data/raw/test_cholesterol.xlsx'
      - 'ARISA_DSML/predict.py'
      - 'models/cholesterol-pred-bclass.cbm'
  workflow_dispatch:

jobs:
  preprocess:
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
        working-directory: ${{ github.workspace }}
    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}
      AZURE_DBSERVER: ${{ secrets.AZURE_DBSERVER }}
      AZURE_DBNAME: ${{ secrets.AZURE_DBNAME }}
      AZURE_DBUSERNAME: ${{ secrets.AZURE_DBUSERNAME }}
      AZURE_DBUSERPASS: ${{ secrets.AZURE_DBUSERPASS }}
      AZURE_DBPORT: ${{ secrets.AZURE_DBPORT || '1433' }}
      AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
      AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
      AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with: 
          python-version: "3.11"
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip

      - name: Create directory structure
        run: mkdir -p data/{raw,processed} models

      - name: Configure Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          make requirements
#preproc
      - name: Run data preprocessing
        run: |
          make preprocess
      - name: Run MLflow server
        run: |
          SQL_CONNECTION_STRING="mssql+pyodbc://${{ env.AZURE_DBUSERNAME }}:${{ env.AZURE_DBUSERPASS }}@${{ env.AZURE_DBSERVER }}:${{ env.AZURE_DBPORT }}/${{ env.AZURE_DBNAME }}?driver=ODBC+Driver+17+for+SQL+Server"
        
          mlflow server \
          --backend-store-uri "$SQL_CONNECTION_STRING" \
          --default-artifact-root "wasbs://${{ env.AZURE_STORAGE_CONTAINER }}@${{ env.AZURE_STORAGE_ACCOUNT }}.blob.core.windows.net/mlruns" \
          --host 0.0.0.0 --port 5000 &
#train
      - name: Train model
        run: |
          make train
          echo "Model generation verification:"
          ls -l models/cholesterol-pred-bclass.cbm

      - name: Validate artifacts
        run: |
          [ -f models/cholesterol-pred-bclass.cbm ] || { echo "Model file missing!"; exit 1; }
          [ -f data/processed/test_cholesterol.xlsx ] || { echo "Processed data missing!"; exit 1; }

      - name: Log model to MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        run: |
          echo "MLflow Version: $(mlflow --version)"
          python ARISA_DSML/log_model.py

      - name: Archive artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-artifacts
          path: |
            data/processed/
            models/
          retention-days: 7

  # Predict job remains unchanged
