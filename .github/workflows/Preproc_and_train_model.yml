name: Cholesterol Prediction Pipeline

on: 
  push:
    branches: [main]
    paths:
      - 'data/raw/test_cholesterol.xlsx'
      - 'ARISA_DSML/predict.py'
      - 'models/cholesterol-pred-bclass.cbm'
  workflow_dispatch:

jobs:
  preprocess:
    runs-on: ubuntu-latest
    permissions:
      id-token: write        # Wymagane dla OIDC
      contents: read         # Wymagane do checkoutu kodu

    defaults:
      run:
        shell: bash
        working-directory: ${{ github.workspace }}

    env:
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}
      AZURE_DBSERVER: ${{ secrets.AZURE_DBSERVER }}
      AZURE_DBNAME: ${{ secrets.AZURE_DBNAME }}
      AZURE_DBUSERNAME: ${{ secrets.AZURE_DBUSERNAME }}
      AZURE_DBUSERPASS: ${{ secrets.AZURE_DBUSERPASS }}
      AZURE_DBPORT: ${{ secrets.AZURE_DBPORT }}
      AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
      AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
      AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with: 
          python-version: "3.11"
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip

      - name: Create directory structure
        run: mkdir -p data/{raw,processed} models

      - name: Configure Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Install coreutils and ODBC dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl gnupg coreutils unixodbc-dev
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo gpg --dearmor -o /usr/share/keyrings/microsoft.gpg
          echo "deb [arch=amd64 signed-by=/usr/share/keyrings/microsoft.gpg] https://packages.microsoft.com/ubuntu/22.04/prod jammy main" | sudo tee /etc/apt/sources.list.d/mssql-release.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18
          echo "export PATH=\$PATH:/opt/mssql-tools18/bin" | sudo tee -a /etc/profile
          source /etc/profile
      
      - name: Configure ODBC
        run: |
          cat <<EOF | sudo tee /etc/odbcinst.ini
          [ODBC Driver 18 for SQL Server]
          Description=Microsoft ODBC Driver 18 for SQL Server
          Driver=/opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.5.so.1.1
          UsageCount=1
          EOF
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow==2.13.1 pyodbc==4.0.39 azure-storage-blob==12.19.1
          pip install loguru kaggle dotenv openpyxl catboost optuna azure-identity azure-storage-blob nannyml traitlets
      
      - name: Verify ODBC driver installation
        run: |
          if ! odbcinst -q -d -n "ODBC Driver 18 for SQL Server"; then
            echo "❌ ODBC Driver 18 NOT found!"
            exit 1
          else
            echo "✅ ODBC Driver 18 verified"
          fi
          if ! python -c "import pyodbc; print('Drivers:', pyodbc.drivers())"; then
            echo "❌ Python/pyodbc problem!"
            exit 1
          else
            echo "✅ Python/pyodbc OK"
          fi

      - name: Azure login with OIDC
        uses: azure/login@v2
        with:
          client-id: ${{ env.AZURE_CLIENT_ID }}
          tenant-id: ${{ env.AZURE_TENANT_ID }}
          subscription-id: ${{ env.AZURE_SUBSCRIPTION_ID }}
          enable-AzPSSession: true

      - name: Verify Azure login
        run: |
          az account show
          az storage account show --name ${{ env.AZURE_STORAGE_ACCOUNT }}

      - name: Run MLflow server
        run: |
          SQL_CONNECTION_STRING="mssql+pyodbc://${{ env.AZURE_DBUSERNAME }}:${{ env.AZURE_DBUSERPASS }}@${{ env.AZURE_DBSERVER }}:${{ env.AZURE_DBPORT }}/${{ env.AZURE_DBNAME }}?Encrypt=yes&TrustServerCertificate=no&driver=ODBC+Driver+18+for+SQL+Server"
          mlflow server \
            --backend-store-uri "$SQL_CONNECTION_STRING" \
            --default-artifact-root "wasbs://${{ env.AZURE_STORAGE_CONTAINER }}@${{ env.AZURE_STORAGE_ACCOUNT }}.blob.core.windows.net/mlruns" \
            --host 0.0.0.0 --port 5000 &
        env:
          AZURE_STORAGE_ACCESS_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
          AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}

      - name: Run data preprocessing
        run: |
          make preprocess

      - name: Train model
        run: |
          make train
          echo "Model generation verification:"
          ls -l models/cholesterol-pred-bclass.cbm

      - name: Validate artifacts
        run: |
          [ -f models/cholesterol-pred-bclass.cbm ] || { echo "Model file missing!"; exit 1; }
          [ -f data/processed/test_cholesterol.xlsx ] || { echo "Processed data missing!"; exit 1; }

      - name: Log model to MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        run: |
          echo "MLflow Version: $(mlflow --version)"
          python ARISA_DSML/log_model.py

      - name: Archive artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-artifacts
          path: |
            data/processed/
            models/
          retention-days: 7
